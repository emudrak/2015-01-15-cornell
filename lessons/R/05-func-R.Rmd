---
layout: topic
title: Analyzing and Plotting Data
minutes: 60
---



```{r, echo=FALSE}
knitr::opts_chunk$set(results='hide', comment=NA)
```
# Creating Functions

If we only had one data set to analyze, it would probably be faster to load the file into a spreadsheet and use that to plot some simple statistics. 
But we have twelve files to check, and may have more in future. In this lesson, we'll learn how to write a function so that we can repeat several operations with a single command.




## Objectives

* Define a function that takes parameters.
* Return a value from a function.
* Test and debug a function.
* Explain what a call stack is, and trace changes to the call stack as functions are called.
* Set default values for function parameters.
* Explain why we should divide programs into small, single-purpose functions.
* Defining a Function




## Common statistical Problem: 
Your data is non-normal, and you need to make it normal for statistical analysis. 
Often, people turn to log-transform. 

```{r, echo=FALSE, fig.height=6}
set.seed(25)
zerodat<-round(rlnorm(50, log(3.5),1.2), 4)-1
zerodat[zerodat<0]<-0

zerodat <- sort(zerodat)
zerodat

par(mfrow=c(2,1), mar=c(5,4,0.5,2))
hist(zerodat, breaks=10, main=NA)
hist(log(zerodat), breaks=10, main=NA)
nonzerodat <- zerodat[zerodat>0]
```

But, your data has some zeros, which are undefined after a log tranform: 
 

```{r, echo=FALSE, results='show'}
cbind(data=zerodat, logdata=log(zerodat))[1:20,]
```

A common solution is to add one to the data. This keeps the zero values in the data, but the shape of the entire curve changes.  The left graph shows the log-transformed data, which strips the zero values.  The right graph shows what happens when we log(x+1). The zero-value is marked with a red asterisk.

```{r, echo=FALSE}
par(mfrow=c(1,2), mar=c(5,4,0.5,2))
plot(zerodat,log(zerodat))
points(nonzerodat[1],log(nonzerodat[1]), col="blue", pch=16)
points(nonzerodat[2],log(nonzerodat[2]), col="orange", pch=16)
points(nonzerodat[3],log(nonzerodat[3]), col="green", pch=16)
plot(zerodat,log(zerodat+1))
points(0,log(1), col="red", pch="*", cex=3)
points(nonzerodat[1],log(nonzerodat[1]+1), col="blue", pch=16)
points(nonzerodat[2],log(nonzerodat[2]+1), col="orange", pch=16)
points(nonzerodat[3],log(nonzerodat[3]+1), col="green", pch=16)
```

The lowest three values are marked in blue, orange and green. Note on the left how spread out they are, and that the orange point is closer to the green point than the blue point.  On the right, they are squished close together. 

This is due to the fact that the smallest non-zero value is equal to `r nonzerodat[1]`. Adding 1 to this value drastically changes it's magnitude, but adding 1 to the maximum value `r max(zerodat)` barely changes it  at all. 


```{r, echo=FALSE}
zerodat

```


Maybe adding 1 is too much in this case.  Perhaps something smaller?  Try 0.01. 
```{r, echo=FALSE}

#Maybe adding 1 is too much. Add something smaller?
add=0.01
par(mfrow=c(1,2), mar=c(5,4,0.5,2))
plot(zerodat,log(zerodat), ylim=c(log(min(nonzerodat)), log(max(zerodat)+add)))
points(nonzerodat[1],log(nonzerodat[1]), col="blue", pch=16)
points(nonzerodat[2],log(nonzerodat[2]), col="orange", pch=16)
points(nonzerodat[3],log(nonzerodat[3]), col="green", pch=16)

plot(zerodat,log(zerodat+add), ylim=c(log(add), log(max(zerodat)+add)), ylab=paste("log(zerodat) +", add))
points(0,log(add), col="red", pch="*", cex=3)
points(nonzerodat[1],log(nonzerodat[1]+add), col="blue", pch=16)
points(nonzerodat[2],log(nonzerodat[2]+add), col="orange", pch=16)
points(nonzerodat[3],log(nonzerodat[3]+add), col="green", pch=16)

```

That is better, but not orange is now midway between blue and green, but blue should be farther. Add something even smaller- Many people like to add half the smallest non-zero value. 

```{r, echo=FALSE}

add=0.001
par(mfrow=c(1,2), mar=c(5,4,0.5,2))
plot(zerodat,log(zerodat), ylim=c(log(min(nonzerodat)), log(max(zerodat)+add)))
points(nonzerodat[1],log(nonzerodat[1]), col="blue", pch=16)
points(nonzerodat[2],log(nonzerodat[2]), col="orange", pch=16)
points(nonzerodat[3],log(nonzerodat[3]), col="green", pch=16)

plot(zerodat,log(zerodat+add), ylim=c(log(add), log(max(zerodat)+add)), ylab=paste("log(zerodat) +", add))
points(0,log(add), col="red", pch="*", cex=3)
points(nonzerodat[1],log(nonzerodat[1]+add), col="blue", pch=16)
points(nonzerodat[2],log(nonzerodat[2]+add), col="orange", pch=16)
points(nonzerodat[3],log(nonzerodat[3]+add), col="green", pch=16)
```

How to determine the correct small value to add? 

McCune and Grace 2002 (Chapt 9) suggest this method to figured out a small value that 
1. Preserves original orders of magnitude in teh data
1. Results in values of zero when initial value is zero. 


Given

  * Min(x) is the smallest nonzero value in the data
  * Int(x) is a function that truncates x to an integer by dropping digits after the decimal point
  * c = order of magnitude constant = Int(log(Min(x))
  * d = decimal constant = log-1 (c) 

then the transformation is $b_{i} = log(x_{i} + d) - c$

Try it out:  First, get the data that we've been working with. 

```{r}

zerodat<-c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.002899, 0.0419, 
0.1719, 0.1868, 0.4309, 0.4459, 0.4809, 0.8135, 0.8976, 0.9305, 
1.0506, 1.7144, 2.2262, 2.2651, 2.303, 2.7367, 2.799, 2.8651, 
2.9446, 3.2168, 3.2858, 3.7499, 3.9239, 4.0544, 4.148, 4.3406, 
4.4707, 5.4644, 5.8323, 8.0209, 8.9348, 9.3409, 9.6532, 10.1077, 
12.8205, 21.3723, 27.0396, 58.9842)


c <- floor(log(min(zerodat[zerodat!=0])))
d <- exp(c)
transf <- log(zerodat+d) - c



```

Now let's plot the data to see how it looks: 


```{r}
par(mfrow=c(1,2), mar=c(3,3,3,3))
plot(zerodat,log(zerodat), ylim=c(log(min(nonzerodat)), log(max(zerodat)+add)))
points(zerodat[13],log(zerodat[13]), col="blue", pch=16)
points(zerodat[14],log(zerodat[14]), col="orange", pch=16)
points(zerodat[15],log(zerodat[15]), col="green", pch=16)

plot(zerodat,transf)
points(0,transf[1], col="red", pch=8)
points(zerodat[13],transf[13], col="blue", pch=16)
points(zerodat[14],transf[14], col="orange", pch=16)
points(zerodat[15],transf[15], col="green", pch=16)
```



So now we have a series of three lines that will take a set of data with zeros and log transform it so that order of magnitues are preserved and zero maps to zero: 


```{r}

c <- floor(log(min(zerodat[zerodat!=0])))
d <- exp(c)
transf <- log(zerodat+d) - c

```

So this set of equations should be applicable to any set of numbers that you might want to log-transform. 


```{r, eval=FALSE, echo=FALSE}
#Create these data sets
set.seed(25)
zero1 <- round(rlnorm(50, log(2.5),1.2), 4)-1
zero1[zero1<0]<-0
zero2 <- round(rlnorm(35, log(8),4), 4)-1
zero2[zero2<0]<-0
zero3 <- round(rlnorm(100, log(6),2.2), 4)-1
zero3[zero3<0]<-0
zero4 <- round(rlnorm(20, log(15),3.5), 4)-1
zero4[zero4<0]<-0

# zero[zero<0]<-0
# zero <- sort(zero)
# zero
# hist(zero)
# hist(log(zero))
# sort(zero)


write.csv(zero1, file="ZeroData1.csv", row.names=FALSE)
write.csv(zero2, file="ZeroData2.csv", row.names=FALSE)
write.csv(zero3, file="ZeroData3.csv", row.names=FALSE)
write.csv(zero4, file="ZeroData4.csv", row.names=FALSE)


```

In the file "DataWithZeros.xlsx" there are several sheets with data that should be log-transformed but have zeros. 

Though you should avoid storing data like this, some researchers receive data this way in such large volumes that it is difficult to "save-as" to .csv files. The following code is provided to show how to work with data in Excel only in these rare circumstances...

```{r, message=FALSE}
library(XLConnect)

DataW0 <- loadWorkbook("../../data/DataWithZeros.xlsx")
zero1 <- readWorksheet(DataW0, sheet="Sheet1")
zero2 <- readWorksheet(DataW0, sheet="Second")
zero3 <- readWorksheet(DataW0, sheet="Sheet3")
zero4 <- readWorksheet(DataW0, sheet="Sheet4")

```

Now we have four data sets to which we can apply the log-transform algorithm. Our first thought might be to find-replace: look for instances where we used to have `zerodat` and replace with the new data `zero1`, `zero1`, etc... 

```{r, results='show'}

c <- floor(log(min(zerodat[zerodat!=0])))
d <- exp(c)
transf <- log(zerodat+d) - c


c <- floor(log(min(zero1[zero1!=0])))
d <- exp(c)
transf1 <- log(zero1+d) - c

cbind(zero1, transf1)[1:10,]

```

Ok, that seemed to work. 

```{r, error=TRUE}


c <- floor(log(min(zero2[zerodat!=0])))
d <- exp(c)
transf2 <- log(zero2+d) - c

cbind(zero2, transf2)[1:10,]

```

Argh.  We made a find-replace error.  We forgot to replace the second instance of `zerodat` in the first line, and R threw and error. 

This illustrates a problem with this approach.  It is very easy to make a mistake in copying.  Aslo, R only made an error because `zerodat` has a length of `r length(zerodat)` and `zero2` has a length of `r nrow(zero2)`, so the indexing was off.  Had the length of the numbers been the same, or had there not been any non-zero elements in positions 36:50 of zerodat, this particular error would not have been thrown, but we still would have gotten bogus data. 

A better option would be to write the lines of code in a generic way, referencing a generic variable, say `myzerodata` and let that variable equal each data object in turn: 


```{r, results="show"}
myzerodata <- zero1
c <- floor(log(min(myzerodata[myzerodata!=0])))
d <- exp(c)
mytransfdata <- log(myzerodata+d) - c

cbind(myzerodata, mytransfdata)[1:10,]


myzerodata <- zero2
c <- floor(log(min(myzerodata[myzerodata!=0])))
d <- exp(c)
mytransfdata <- log(myzerodata+d) - c

cbind(myzerodata, mytransfdata)[1:10,]

```

That works much better, because there is less of a chance of a find-replace error. 

But, this still takes up three lines of code, and for a data transformation in the middle of an analysis, I think that's too much. 

The beauty of using R for you data analysis is that you can also use it to program, and this allows you to have much more efficient code.  

# Writing a function

> To prepare for the next part of the lesson, clear your workspace, either by typing `rm(list=ls())` or by pressing the "Clear" button with the broom picture on the Environment Tab. 

We can use these three as the body of a function that we name `logw0`: 

```{r}

logw0 <- function (myzerodata){
  #This function log-transforms a set of numbers that may include zeros in a way that preserves the original order of magnitude and maps zero to zero.  It follows McCune and Grace 2002 (Page 69). 
  # myzerodata is a numeric vector
  
  c <- floor(log(min(myzerodata[myzerodata!=0])))
  d <- exp(c)
  mytransfdata <- log(myzerodata+d) - c
  return(mytransfdata)
}

```

A function in R is named object, takes arguments as specified in the parentheses, and executes the set of commands between the curly brackets `{ }`. Note the body of the function is indented to aid in readability.  The closing bracket lines up vertically with the function name. Always provide enough documentation at the beginning of the function. 


Now we can see if our function works on our data.  Let's read it in again (since we lost it when we cleared the workspace): 

```{r, message=FALSE}
library(XLConnect)

DataW0 <- loadWorkbook("../../data/DataWithZeros.xlsx")
zero1 <- readWorksheet(DataW0, sheet="Sheet1")$x
zero2 <- readWorksheet(DataW0, sheet="Second")$x
zero3 <- readWorksheet(DataW0, sheet="Sheet3")$x
zero4 <- readWorksheet(DataW0, sheet="Sheet4")$x

```

Now we can call the function in a single line with any of our data sets: 


```{r , results="show"}

logw0(zero1)
logw0(zero2)

```

We can also use it in the middle of other commands, just like we used the `log()` funtion: 

```{r}

plot(zero3, logw0(zero3))

```

This is a much cleaner way to work with this way of log-transforming, particularly if we have to do it with a lot of data in our workspace. 

* Note* Look in the environment tab.  There are no intermediate variables `c` and `d`.  R executes the lines inside of the `logw0` function and creates these variables temporarily and stores it in somthing called a `stack frame` taht only exists for the duration of time that R needs to execute the function.  Then it throws those variables away. This is a great way to keep your environment clean and free from all the temporary variables. 

* Note* This function shows up in the Environment tab, and it is like any object.  Before you use it in an analysis, you have to call it.  Typing the name of the function will print out it's contents, which we just wrote: 

```{r }
logw0
```

If you are writing and using a lot of custom functions, it can be cumbersome to fill up the first half of your analysis script by defining all these functions.  An easy away around this is to save it in a separate R-script (File --> New File --> R Script), say we name it "LogFunction.R". 

Then at the beginning of a script, we can load this function in a single line with the code  
```{r}
source("LogFunction.R")
```

The `source` function will execute all commands in that file quietly, so you don't have to see it working.  


### Challenges

This next challenge has several steps. Think about how you break down a difficult problem into manageable pieces.

1. Write a function called `analyze` that takes a filename as a parameter and displays the 3 graphs you made earlier (average, min and max inflammation over time). i.e., `analyze('data/inflammation-01.csv')` should produce the graphs already shown, while `analyze('inflammation-02.csv')` should produce corresponding graphs for the second data set. Be sure to give your function a docstring.



```{r results="hide"}
dat <- read.csv(file="../../data/inflammation/inflammation-01.csv")
```



```{r,  eval=FALSE}
dat <- read.csv(file = "inflammation-01.csv", header = FALSE)

head(dat)

```

```{r}

analyze <- function(filename){
  # x is a file name for a data frame of numbers
  # This function reads the csv file with inflammation data, calculates the mean, max and min inflmmation values across patients for each day, and then plots it side by side
  dat <- read.csv(file = filename, header = FALSE)
  avg_day_inflammation <- apply(dat, 2, mean)
  max_day_inflammation <- apply(dat, 2, max)
  min_day_inflammation <- apply(dat, 2, min)
  
  par(mfrow=c(1,3))
  plot(avg_day_inflammation, type="l")
  plot(max_day_inflammation, type="l")
  plot(min_day_inflammation, type="l")

}


```

Try it out: 



## Defining Defaults
```{r }

analyze("../../data/inflammation/inflammation-01.csv")
analyze("../../data/inflammation/inflammation-02.csv")
analyze("../../data/inflammation/inflammation-03.csv")
analyze("../../data/inflammation/inflammation-04.csv")
```


## Next Steps

We now have a function called analyze to visualize a single data set. We could use it to explore all 12 of our current data sets like this:

```{r, eval=FALSE}
# analyze('data/inflammation-01.csv')
# analyze('data/inflammation-02.csv')
# #...
# analyze('data/inflammation-12.csv')
```

but the chances of us typing all 12 filenames correctly aren't great, and we'll be even worse off if we get another hundred files. What we need is a way to tell R to do something once for each file, and that will be the subject of the next lesson.

--->
